{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "daQHIra_w5H_"
      },
      "outputs": [],
      "source": [
        "!ssh-keygen -t rsa -b 4096"
      ],
      "id": "daQHIra_w5H_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hp-x-DRIKkNc"
      },
      "outputs": [],
      "source": [
        "!ssh-keyscan -t rsa github.com >> ~/.ssh/known_hosts"
      ],
      "id": "hp-x-DRIKkNc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ip71BoQyKolA"
      },
      "outputs": [],
      "source": [
        "!cat /root/.ssh/id_rsa.pub"
      ],
      "id": "Ip71BoQyKolA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "akAZCymeKocY"
      },
      "outputs": [],
      "source": [
        "!ssh -T git@github.com"
      ],
      "id": "akAZCymeKocY"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KvAj1yPgKoSr"
      },
      "outputs": [],
      "source": [
        "!git clone git@github.com:AmitB30/cmr-haste"
      ],
      "id": "KvAj1yPgKoSr"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "arVV8_qCLAbm"
      },
      "outputs": [],
      "source": [
        "%cd cmr-haste"
      ],
      "id": "arVV8_qCLAbm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zeq8AL5ULC0W"
      },
      "outputs": [],
      "source": [
        "!pip install -e."
      ],
      "id": "Zeq8AL5ULC0W"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "461r-O8IvoGP"
      },
      "outputs": [],
      "source": [
        "!pip install monai\n",
        "!pip install timm\n",
        "!pip install tqdm\n",
        "\n",
        "import argparse\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "from datetime import datetime\n",
        "from uuid import uuid4\n",
        "from collections import OrderedDict\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import monai as M\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "from more_itertools import flatten\n",
        "from monai.utils import set_determinism\n",
        "from monai.metrics import DiceMetric, HausdorffDistanceMetric\n",
        "from monai.networks.nets import UNet, BasicUNet\n",
        "from monai.networks.utils import one_hot\n",
        "from cv2 import findContours\n",
        "from sklearn.model_selection import LeaveOneOut, KFold\n",
        "\n",
        "from cmr_haste import utils\n",
        "from cmr_haste.data import ImageDataset, ImageSegDataset, setup_data\n",
        "from cmr_haste.losses import DiceLoss, dice_score\n",
        "\n",
        "try:\n",
        "    from torch.utils.tensorboard import SummaryWriter\n",
        "    has_tb = True\n",
        "except ImportError:\n",
        "    has_tb = False\n",
        "    \n",
        "_logger = logging.getLogger('train')"
      ],
      "id": "461r-O8IvoGP"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKqWf3XCvoGX"
      },
      "source": [
        "#### Setup configuration and environment"
      ],
      "id": "RKqWf3XCvoGX"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRxW48sAEkVW",
        "outputId": "ba417695-1865-42a7-a875-28dd3bf49e16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount GDrive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "PRxW48sAEkVW"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "OyzRr9JGvoGa"
      },
      "outputs": [],
      "source": [
        "dataset_dir = '/content/drive/My Drive/cmr-haste-main/cmr-haste-data'\n",
        "out_root = '/content/drive/My Drive/cmr-haste-main/output/'\n",
        "exp_name = '-'.join([datetime.now().strftime(\"%Y%m%d-%H%M%S\"), 'seg-haste'])\n",
        "exp_uid = str(uuid4)\n",
        "resume = None\n",
        "log_tb = True\n",
        "device = 'cuda'\n",
        "global_seed = 0\n",
        "dataset_seed = 0\n",
        "center_crop_size = (320, 320)\n",
        "pad_size = (320, 320)\n",
        "spatial_size = (320, 320)\n",
        "rand_affine_prob = 0.3\n",
        "rotate_range = 0.31415\n",
        "shear_range = None\n",
        "translate_range = 30\n",
        "scale_range = 0.2\n",
        "batch_size = 32\n",
        "num_workers = 24\n",
        "pin_memory = True\n",
        "in_channels = 1\n",
        "out_channels = 1\n",
        "num_features = [16, 32, 64, 128, 256, 16]\n",
        "lr = 0.001\n",
        "weight_decay = 0.0001\n",
        "seg_mode = 'binary'\n",
        "val_metric = 'loss'\n",
        "recovery_interval = None\n",
        "num_epochs = 200"
      ],
      "id": "OyzRr9JGvoGa"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvJMb36PvoGc"
      },
      "source": [
        "Set the global seed to ensure reproducibility:"
      ],
      "id": "lvJMb36PvoGc"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "qyGMNOeRvoGd"
      },
      "outputs": [],
      "source": [
        "set_determinism(global_seed)"
      ],
      "id": "qyGMNOeRvoGd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VgNDyKAvoGe"
      },
      "source": [
        "Setup output directories, logging, and tensorboard:"
      ],
      "id": "9VgNDyKAvoGe"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "QqZvxArPvoGf"
      },
      "outputs": [],
      "source": [
        "output_dir, logs_dir, checkpoint_dir, recovery_dir = utils.get_out_dirs(out_root, exp_name)\n",
        "utils.setup_default_logging(log_path=os.path.join(logs_dir, f'{exp_name}.log'))"
      ],
      "id": "QqZvxArPvoGf"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "VqeENkckvoGg"
      },
      "outputs": [],
      "source": [
        "writer = None\n",
        "if log_tb:\n",
        "    if has_tb:\n",
        "        writer = SummaryWriter(log_dir=logs_dir)\n",
        "    else:\n",
        "        _logger.warning(\"You've requested to log to tensorboard but module not found. \"\n",
        "                        \"Nothing will be logged to tensorboard, try `pip install tensorboard`\")"
      ],
      "id": "VqeENkckvoGg"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BschGk54voGh"
      },
      "source": [
        "#### Setup data and transforms"
      ],
      "id": "BschGk54voGh"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "nnomIHpnvoGi"
      },
      "outputs": [],
      "source": [
        "keys = ['image', 'seg']\n",
        "transforms_train = M.transforms.Compose([\n",
        "    M.transforms.AddChannelD(keys=keys),\n",
        "    M.transforms.CenterSpatialCropD(keys=keys, roi_size=center_crop_size),\n",
        "    M.transforms.SpatialPadD(keys=keys, spatial_size=pad_size),\n",
        "    M.transforms.ResizeD(keys=keys, spatial_size=spatial_size, mode=['bilinear', 'nearest'], align_corners=[False, None]),\n",
        "    M.transforms.ScaleIntensityD(keys=['image', 'seg'], minv=0, maxv=1),\n",
        "    M.transforms.ToTensorD(keys=keys),\n",
        "\n",
        "    M.transforms.RandRotated(keys=keys, range_x=90,prob=0.3,keep_size=True),\n",
        "    M.transforms.RandAxisFlipd(keys=keys, prob=0.3),\n",
        "    M.transforms.RandZoomd(keys=keys,min_zoom=0.9, max_zoom=1.5, prob=0.3),\n",
        "\n",
        "])\n",
        "transforms_test = M.transforms.Compose([\n",
        "    M.transforms.AddChannelD(keys=keys),\n",
        "    M.transforms.CenterSpatialCropD(keys=keys, roi_size=center_crop_size),\n",
        "    M.transforms.SpatialPadD(keys=keys, spatial_size=pad_size),\n",
        "    M.transforms.ResizeD(keys=keys, spatial_size=spatial_size, mode=['bilinear', 'nearest'], align_corners=[False, None]),\n",
        "    M.transforms.ScaleIntensityD(keys=['image', 'seg'], minv=0, maxv=1),\n",
        "    M.transforms.ToTensorD(keys=keys),\n",
        "])"
      ],
      "id": "nnomIHpnvoGi"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "RytkiJyVvoGj"
      },
      "outputs": [],
      "source": [
        "with open(os.path.join(dataset_dir, 'metadata.json'), 'r') as f:\n",
        "    meta = json.load(f)\n",
        "data = meta['data']"
      ],
      "id": "RytkiJyVvoGj"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "h9mnfGulvoGk"
      },
      "outputs": [],
      "source": [
        "dataset_train, dataset_val, dataset_test = setup_data(\n",
        "    root=dataset_dir,\n",
        "    partition_data=True,\n",
        "    train_split=0.5,\n",
        "    val_split=0.25,\n",
        "    test_split=0.25,\n",
        "    train_transforms=transforms_train,\n",
        "    val_transforms=transforms_test,\n",
        "    test_transforms=transforms_test,\n",
        "    seed=dataset_seed,\n",
        ")"
      ],
      "id": "h9mnfGulvoGk"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n5mvbsX_ZV_S"
      },
      "outputs": [],
      "source": [
        "#Get number of images/ patients in data sets\n",
        "train_studies = []\n",
        "val_studies = []\n",
        "test_studies = []\n",
        "\n",
        "i=0\n",
        "for item in dataset_train.data:\n",
        "  if dataset_train.data[i]['meta']['study_name'] not in train_studies:\n",
        "    train_studies.append(dataset_train.data[i]['meta']['study_name'])\n",
        "  i+=1\n",
        "print(\"Train set contains\", str(len(train_studies)), \"patients:\", train_studies)\n",
        "print(\"Train set contains\", str(len(dataset_train.data)), \"images\") #205 images\n",
        "print(\"\\n\")\n",
        "\n",
        "i=0\n",
        "for item in dataset_val.data:\n",
        "  if dataset_val.data[i]['meta']['study_name'] not in val_studies:\n",
        "    val_studies.append(dataset_val.data[i]['meta']['study_name'])\n",
        "  i+=1\n",
        "print(\"Val set contains\", str(len(val_studies)), \"patients:\", val_studies)\n",
        "print(\"Val set contains\", str(len(dataset_val.data)), \"images\") #205 images\n",
        "print(\"\\n\")\n",
        "\n",
        "i=0\n",
        "for item in dataset_test.data:\n",
        "  if dataset_test.data[i]['meta']['study_name'] not in test_studies:\n",
        "    test_studies.append(dataset_test.data[i]['meta']['study_name'])\n",
        "  i+=1\n",
        "print(\"Test set contains\", str(len(test_studies)), \"patients:\", test_studies)\n",
        "print(\"Test set contains\", str(len(dataset_test.data)), \"images\") #48 images"
      ],
      "id": "n5mvbsX_ZV_S"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ja_YwEWt0Bvv"
      },
      "outputs": [],
      "source": [
        "#Plot Datasets\n",
        "\"\"\"\n",
        "# Plot train set\n",
        "fig, axes = plt.subplots(16,10, figsize=(25,40))\n",
        "try:\n",
        "  for idx, ax in enumerate(axes.flatten()):\n",
        "      ax.imshow(dataset_train[idx]['image'].squeeze().numpy().T, cmap='gray')\n",
        "      ax.imshow(dataset_train[idx]['seg'].squeeze().numpy().T, alpha=0.3)\n",
        "\n",
        "      ax.set_title(\"image \"+str(idx))\n",
        "      ax.set_axis_off()\n",
        "except IndexError:\n",
        "  pass\n",
        "\n",
        "#Plot val set\n",
        "fig, axes = plt.subplots(9,8, figsize=(40,45))\n",
        "try:\n",
        "  for idx, ax in enumerate(axes.flatten()):\n",
        "      ax.imshow(dataset_val[idx]['image'].squeeze().numpy().T, cmap='gray')\n",
        "      ax.imshow(dataset_val[idx]['seg'].squeeze().numpy().T, alpha=0.3)\n",
        "\n",
        "      ax.set_title(\"image \"+str(idx))\n",
        "      ax.set_axis_off()\n",
        "except IndexError:\n",
        "  pass\n",
        "\n",
        "#Plot test set\n",
        "fig, axes = plt.subplots(9,8, figsize=(40,45))\n",
        "try:\n",
        "  for idx, ax in enumerate(axes.flatten()):\n",
        "      ax.imshow(dataset_test[idx]['image'].squeeze().numpy().T, cmap='gray')\n",
        "      ax.imshow(dataset_test[idx]['seg'].squeeze().numpy().T, alpha=0.3)\n",
        "\n",
        "      ax.set_title(\"image \"+str(idx))\n",
        "      ax.set_axis_off()\n",
        "except IndexError:\n",
        "  pass\n",
        "\n",
        "\"\"\""
      ],
      "id": "ja_YwEWt0Bvv"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rIoN1XGcvoGl"
      },
      "outputs": [],
      "source": [
        "dataloader_train = M.data.DataLoader(\n",
        "    dataset_train,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=pin_memory,\n",
        ")\n",
        "dataloader_val = M.data.DataLoader(\n",
        "    dataset_val,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=pin_memory,\n",
        ")\n",
        "dataloader_test = M.data.DataLoader(\n",
        "    dataset_test,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=pin_memory,\n",
        ")"
      ],
      "id": "rIoN1XGcvoGl"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgR2L-eHvoGl"
      },
      "source": [
        "#####We'll plot some images to check image loading is working correctly:"
      ],
      "id": "bgR2L-eHvoGl"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CvrZZbpKvoGm"
      },
      "outputs": [],
      "source": [
        "train_batch = next(iter(dataloader_train))"
      ],
      "id": "CvrZZbpKvoGm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AaEHazDxvoGm"
      },
      "outputs": [],
      "source": [
        "#Plotting a batch of data from train set\n",
        "fig, axes = plt.subplots(5,7, figsize=(28, 20))\n",
        "try:\n",
        "  for idx, ax in enumerate(axes.flatten()):\n",
        "      #ax.imshow(np.moveaxis(batch['image'][idx].cpu().numpy(), 0, -1), cmap='gray')\n",
        "      #ax.imshow(np.moveaxis(batch['seg'][idx].cpu().numpy(), 0, -1), alpha=0.3)\n",
        "      ax.imshow(train_batch['image'][idx].cpu().squeeze().numpy().T, cmap='gray')\n",
        "      ax.imshow(train_batch['seg'][idx].cpu().squeeze().numpy().T, alpha=0.3)\n",
        "      ax.set_title('train '+str(idx))\n",
        "      #you could also look at plt.contour()\n",
        "      ax.set_axis_off()\n",
        "except IndexError:\n",
        "  pass"
      ],
      "id": "AaEHazDxvoGm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wNI8ZrGFhqLi"
      },
      "outputs": [],
      "source": [
        "#Plotting a batch of data from validation set\n",
        "\"\"\"\n",
        "val_batch = next(iter(dataloader_val))\n",
        "\n",
        "fig, axes = plt.subplots(5,7, figsize=(28, 20))\n",
        "try:\n",
        "  for idx, ax in enumerate(axes.flatten()):\n",
        "      #ax.imshow(np.moveaxis(batch['image'][idx].cpu().numpy(), 0, -1), cmap='gray')\n",
        "      #ax.imshow(np.moveaxis(batch['seg'][idx].cpu().numpy(), 0, -1), alpha=0.3)\n",
        "      ax.set_title('val '+str(idx))\n",
        "      ax.imshow(val_batch['image'][idx].cpu().squeeze().numpy().T, cmap='gray')\n",
        "      ax.imshow(val_batch['seg'][idx].cpu().squeeze().numpy().T, alpha=0.3)\n",
        "    \n",
        "      ax.set_axis_off()\n",
        "except IndexError:\n",
        "  pass\n",
        "\"\"\""
      ],
      "id": "wNI8ZrGFhqLi"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AksFutUJvoGn"
      },
      "source": [
        "#### Create the model -- we'll just use an implementation of UNet in MONAI for this"
      ],
      "id": "AksFutUJvoGn"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGNMHc7zvoGn"
      },
      "outputs": [],
      "source": [
        "model = BasicUNet(\n",
        "    spatial_dims=2,\n",
        "    in_channels=in_channels,\n",
        "    out_channels=out_channels,\n",
        "    features=num_features,\n",
        "    act=('LeakyReLU', {'negative_slope': 0.1, 'inplace': True}),\n",
        "    norm=('batch', {}),\n",
        "    bias=True,\n",
        "    dropout=0.0,\n",
        ")\n",
        "model_type = type(model).__name__\n",
        "_logger.info(\n",
        "    f'Model {model_type} created, param count:{sum([m.numel() for m in model.parameters()])}')"
      ],
      "id": "pGNMHc7zvoGn"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6N_ExoJevoGo"
      },
      "source": [
        "#### Create the optimizer and loss functions"
      ],
      "id": "6N_ExoJevoGo"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "RMPvw4qMvoGo"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(\n",
        "    params=model.parameters(),\n",
        "    lr=lr,\n",
        "    weight_decay=weight_decay,\n",
        ")\n",
        "lr_scheduler = None\n",
        "loss_fn_train = DiceLoss(mode=seg_mode)\n",
        "loss_fn_val = DiceLoss(mode=seg_mode)"
      ],
      "id": "RMPvw4qMvoGo"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VulpV6xVvoGp"
      },
      "source": [
        "#### Setup checkpoint saver"
      ],
      "id": "VulpV6xVvoGp"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "j3-N4SLNvoGp"
      },
      "outputs": [],
      "source": [
        "decreasing = True if val_metric == 'loss' else False\n",
        "saver = utils.CheckpointSaver(\n",
        "    model=model,\n",
        "    optimizer=optimizer,\n",
        "    checkpoint_dir=checkpoint_dir,\n",
        "    recovery_dir=recovery_dir,\n",
        "    decreasing=decreasing,\n",
        "    max_history=5,\n",
        ")"
      ],
      "id": "j3-N4SLNvoGp"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FisDAQ8BvoGq"
      },
      "source": [
        "#### Now we train and validate the model"
      ],
      "id": "FisDAQ8BvoGq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0uBsRGTvoGq"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(epoch, model, loader, opt, loss_fn, lr_scheduler=None, saver=None):\n",
        "    \n",
        "    losses_m = utils.AverageMeter()\n",
        "\n",
        "    model.train()\n",
        "    last_idx = len(loader) - 1\n",
        "    num_updates = epoch * len(loader)\n",
        "    prog_bar = tqdm(loader, desc=f'Train epoch {epoch:{\"0\"}{\">\"}{4}}')\n",
        "    for batch_idx, batch in enumerate(prog_bar):\n",
        "        last_batch = batch_idx == last_idx\n",
        "\n",
        "        image, seg = batch['image'].cuda(), batch['seg'].cuda()\n",
        "        output = model(image)\n",
        "        loss = loss_fn(output, seg)\n",
        "        losses_m.update(loss.item(), image.size(0))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "        num_updates += 1\n",
        "\n",
        "        image_filename = (\"train_epoch_\"+str(epoch)+\"_batch_\"+str(batch_idx)+\"_image\")\n",
        "        seg_filename = (\"train_epoch_\"+str(epoch)+\"_batch_\"+str(batch_idx)+\"_seg\")\n",
        "        output_filename = (\"train_epoch_\"+str(epoch)+\"_batch_\"+str(batch_idx)+\"_output\")\n",
        "\n",
        "        image = image.cpu().numpy()\n",
        "        seg = seg.cpu().numpy()\n",
        "\n",
        "        proba = torch.sigmoid(output)\n",
        "        output = (proba >= 0.5) * 1\n",
        "        output = output.cpu().detach().numpy()\n",
        "\n",
        "        np.save(image_filename,image)\n",
        "        np.save(seg_filename,seg)\n",
        "        np.save(output_filename,output)\n",
        "\n",
        "\n",
        "        if last_batch:\n",
        "            pb_str = f'loss: {losses_m.avg:.6f}'\n",
        "            if saver is not None and recovery_interval and (epoch + 1) % recovery_interval == 0:\n",
        "                saver.save_recovery(epoch, batch_idx=batch_idx)\n",
        "            if lr_scheduler is not None:\n",
        "                lr_scheduler.step_update(num_updates=num_updates, metric=losses_m.avg)\n",
        "        else:\n",
        "            pb_str = f'loss: {losses_m.val:.6f}'\n",
        "\n",
        "        prog_bar.set_postfix_str(pb_str)\n",
        "\n",
        "\n",
        "    metrics = OrderedDict([('loss', losses_m.avg)])\n",
        "\n",
        "    return metrics"
      ],
      "id": "u0uBsRGTvoGq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yB35-SuZvoGr"
      },
      "outputs": [],
      "source": [
        "def validate(model, loader, loss_fn):\n",
        "\n",
        "    losses_m = utils.AverageMeter()\n",
        "\n",
        "    model.eval()\n",
        "    last_idx = len(loader) - 1\n",
        "    prog_bar = tqdm(loader, desc=f'{\"Val\":{\"\"}{\">\"}{16}}')\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, batch in enumerate(prog_bar):\n",
        "            last_batch = batch_idx == last_idx\n",
        "\n",
        "            image, seg = batch['image'].cuda(), batch['seg'].cuda()\n",
        "            output = model(image)\n",
        "            loss = loss_fn(output, seg)\n",
        "            losses_m.update(loss.item(), image.size(0))\n",
        "\n",
        "            image_filename = (\"val_epoch_\"+str(epoch)+\"_batch_\"+str(batch_idx)+\"_image\")\n",
        "            seg_filename = (\"val_epoch_\"+str(epoch)+\"_batch_\"+str(batch_idx)+\"_seg\")\n",
        "            output_filename = (\"val_epoch_\"+str(epoch)+\"_batch_\"+str(batch_idx)+\"_output\")\n",
        "\n",
        "            image = image.cpu().numpy()\n",
        "            seg = seg.cpu().numpy()\n",
        "\n",
        "            proba = torch.sigmoid(output)\n",
        "            output = (proba >= 0.5) * 1\n",
        "            output = output.cpu().detach().numpy()\n",
        "\n",
        "            np.save(image_filename,image)\n",
        "            np.save(seg_filename,seg)\n",
        "            np.save(output_filename,output)\n",
        "\n",
        "\n",
        "            if last_batch:\n",
        "                pb_str = f'loss: {losses_m.avg:.6f}'\n",
        "            else:\n",
        "                pb_str = f'loss: {losses_m.val:.6f}'\n",
        "            prog_bar.set_postfix_str(pb_str)\n",
        "\n",
        "    metrics = OrderedDict([('loss', losses_m.avg)])\n",
        "\n",
        "    return metrics"
      ],
      "id": "yB35-SuZvoGr"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M1AbNvzuvoGr"
      },
      "outputs": [],
      "source": [
        "%cd cmr-haste\n",
        "\n",
        "model = model.cuda()\n",
        "metrics_train = []\n",
        "metrics_val = []\n",
        "best_metric = None\n",
        "best_epoch = None\n",
        "best_loss_val = np.inf\n",
        "#num_epochs = 6\n",
        "try:\n",
        "    for epoch in range(0,num_epochs):\n",
        "\n",
        "        train_out = train_one_epoch(epoch, model, dataloader_train, optimizer, loss_fn_train, lr_scheduler, saver)\n",
        "        val_out = validate(model, dataloader_val, loss_fn_val)\n",
        "\n",
        "        train_metrics = train_out\n",
        "        val_metrics = val_out\n",
        "        \n",
        "        metrics_train.append(train_metrics)\n",
        "        metrics_val.append(val_metrics)\n",
        "        \n",
        "\n",
        "        #print(train_metrics)\n",
        "        #print(val_metrics)\n",
        "\n",
        "        if lr_scheduler is not None:\n",
        "            lr_scheduler.step(epoch, val_metrics[val_metric])\n",
        "\n",
        "        if output_dir is not None:\n",
        "            utils.update_summary(\n",
        "                epoch, train_metrics, filename=os.path.join(logs_dir, 'summary.csv'), val_metrics=val_metrics,\n",
        "                write_header=best_metric is None, writer=writer, log_wandb=False)\n",
        "        \n",
        "        if val_metrics['loss'] < best_loss_val:\n",
        "            best_loss_val = val_metrics['loss']\n",
        "            checkpoint = {\n",
        "                'model': model.state_dict(),\n",
        "                'optimizer': optimizer.state_dict(),\n",
        "                'epoch': epoch,\n",
        "                'loss': val_metrics['loss'],\n",
        "              }\n",
        "\n",
        "            save_path = checkpoint_dir + '/model_best_epoch_'+str(epoch)+'.pth'\n",
        "            torch.save(checkpoint, save_path)\n",
        "            best_model_path = save_path\n",
        "            print(best_model_path)\n",
        "        \n",
        "        if saver is not None:\n",
        "            save_metric = val_metrics[val_metric]\n",
        "            best_metric, best_epoch = saver.save_checkpoint(epoch, metric=save_metric)\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    pass\n",
        "\n",
        "if best_metric is not None:\n",
        "    _logger.info(f'*** Best metric: {best_metric:.6f} (epoch {best_epoch})')"
      ],
      "id": "M1AbNvzuvoGr"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Plotting Predictions from our train and validation set:"
      ],
      "metadata": {
        "id": "Xs193Y1ySl8M"
      },
      "id": "Xs193Y1ySl8M"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZwofH-IITOhC"
      },
      "outputs": [],
      "source": [
        "#Train Preds\n",
        "train_epoch_images = []\n",
        "train_epoch_segs = []\n",
        "train_epoch_preds = []\n",
        "\n",
        "start_epoch = 140\n",
        "epochs = 10\n",
        "end_epoch = (start_epoch+epochs)\n",
        "for epoch_num in range(start_epoch,end_epoch):\n",
        "  batch_images = []\n",
        "  batch_segs = []\n",
        "  batch_preds = []  \n",
        "  for i in range (0,4):\n",
        "    image = np.load('train_epoch_'+str(epoch_num)+'_batch_'+str(i)+'_image.npy')\n",
        "    image = image[0,:,:,:]\n",
        "    image = np.moveaxis(image,0,-1)\n",
        "\n",
        "    seg = np.load('train_epoch_'+str(epoch_num)+'_batch_'+str(i)+'_seg.npy')\n",
        "    seg = seg[0,:,:,:]\n",
        "    seg = np.moveaxis(seg,0,-1)\n",
        "\n",
        "    pred = np.load('train_epoch_'+str(epoch_num)+'_batch_'+str(i)+'_output.npy')\n",
        "    pred = pred[0,:,:,:]\n",
        "    pred = np.moveaxis(pred,0,-1)\n",
        "\n",
        "    batch_images.append(image)\n",
        "    batch_segs.append(seg)\n",
        "    batch_preds.append(pred)\n",
        "\n",
        "  train_epoch_images.append(batch_images)\n",
        "  train_epoch_segs.append(batch_segs)\n",
        "  train_epoch_preds.append(batch_preds)\n",
        "\n",
        "epoch_range = end_epoch-start_epoch\n",
        "for epoch_num in range(0,epoch_range):\n",
        "  fig, axes = plt.subplots(1,4, figsize=(30,10))\n",
        "  try:\n",
        "    for idx, ax in enumerate(axes.flatten()):\n",
        "        ax.imshow(train_epoch_images[epoch_num][idx].squeeze().T, cmap='gray')\n",
        "        ax.imshow(train_epoch_segs[epoch_num][idx].squeeze().T, alpha=0.3)\n",
        "        ax.set_title(\"epoch \"+(str(start_epoch+epoch_num))+\" image \"+str(idx))\n",
        "        ax.set_axis_off()\n",
        "  except IndexError:\n",
        "    pass\n",
        "\n",
        "  image_name = \"epoch_\"+str(start_epoch+epoch_num)+\"_input_1\"\n",
        "  #plt.savefig('/content/drive/My Drive/cmr-haste-main/plots/train_input/'+(image_name)+'.png')\n",
        "\n",
        "  fig, axes = plt.subplots(1,4, figsize=(30,10))\n",
        "  try:\n",
        "    for idx, ax in enumerate(axes.flatten()):\n",
        "        ax.imshow(train_epoch_images[epoch_num][idx].squeeze().T, cmap='gray')\n",
        "        ax.imshow(train_epoch_preds[epoch_num][idx].squeeze().T, alpha=0.3)\n",
        "        ax.set_title(\"epoch \"+(str(start_epoch+epoch_num))+\" out \"+str(idx))\n",
        "        ax.set_axis_off()\n",
        "  except IndexError:\n",
        "    pass\n",
        "\n",
        "  image_name = \"epoch_\"+str(start_epoch+epoch_num)+\"_output_1\"\n",
        "  #plt.savefig('/content/drive/My Drive/cmr-haste-main/plots/train_output/'+(image_name)+'.png')\n",
        "\n",
        "\n",
        "#Clear lists to make space in RAM\n",
        "train_epoch_images = []\n",
        "train_epoch_segs = []\n",
        "train_epoch_preds = []\n"
      ],
      "id": "ZwofH-IITOhC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yc39s5w-1IBX"
      },
      "outputs": [],
      "source": [
        "#Val Preds\n",
        "val_epoch_images = []\n",
        "val_epoch_segs = []\n",
        "val_epoch_preds = []\n",
        "\n",
        "start_epoch = 140\n",
        "epochs = 10\n",
        "end_epoch = (start_epoch+epochs)\n",
        "for epoch_num in range(start_epoch,end_epoch):\n",
        "  batch_images = []\n",
        "  batch_segs = []\n",
        "  batch_preds = []  \n",
        "  for i in range (0,2):\n",
        "    image = np.load('val_epoch_'+str(epoch_num)+'_batch_'+str(i)+'_image.npy')\n",
        "    image = image[0,:,:,:]\n",
        "    image = np.moveaxis(image,0,-1)\n",
        "\n",
        "    seg = np.load('val_epoch_'+str(epoch_num)+'_batch_'+str(i)+'_seg.npy')\n",
        "    seg = seg[0,:,:,:]\n",
        "    seg = np.moveaxis(seg,0,-1)\n",
        "\n",
        "    pred = np.load('val_epoch_'+str(epoch_num)+'_batch_'+str(i)+'_output.npy')\n",
        "    pred = pred[0,:,:,:]\n",
        "    pred = np.moveaxis(pred,0,-1)\n",
        "\n",
        "    batch_images.append(image)\n",
        "    batch_segs.append(seg)\n",
        "    batch_preds.append(pred)\n",
        "\n",
        "  val_epoch_images.append(batch_images)\n",
        "  val_epoch_segs.append(batch_segs)\n",
        "  val_epoch_preds.append(batch_preds)\n",
        "\n",
        "\n",
        "epoch_range = end_epoch-start_epoch\n",
        "for epoch_num in range(0,epoch_range):\n",
        "  fig, axes = plt.subplots(1,2, figsize=(20,10))\n",
        "  try:\n",
        "    for idx, ax in enumerate(axes.flatten()):\n",
        "        ax.imshow(val_epoch_images[epoch_num][idx].squeeze().T, cmap='gray')\n",
        "        ax.imshow(val_epoch_segs[epoch_num][idx].squeeze().T, alpha=0.3)\n",
        "        ax.set_title(\"epoch \"+(str(start_epoch+epoch_num))+\" image \"+str(idx))\n",
        "        ax.set_axis_off()\n",
        "  except IndexError:\n",
        "    pass\n",
        "  \n",
        "  image_name = \"val_epoch_\"+str(start_epoch+epoch_num)+\"_input_1\"\n",
        "  #plt.savefig('/content/drive/My Drive/cmr-haste-main/plots/val_input/'+(image_name)+'.png')\n",
        "\n",
        "  fig, axes = plt.subplots(1,2, figsize=(20,10))\n",
        "  try:\n",
        "    for idx, ax in enumerate(axes.flatten()):\n",
        "        ax.imshow(val_epoch_images[epoch_num][idx].squeeze().T, cmap='gray')\n",
        "        ax.imshow(val_epoch_preds[epoch_num][idx].squeeze().T, alpha=0.3)\n",
        "        ax.set_title(\"epoch \"+(str(start_epoch+epoch_num))+\" out \"+str(idx))\n",
        "        ax.set_axis_off()\n",
        "  except IndexError:\n",
        "    pass\n",
        "\n",
        "  image_name = \"val_epoch_\"+str(start_epoch+epoch_num)+\"_output_1\"\n",
        "  #plt.savefig('/content/drive/My Drive/cmr-haste-main/plots/val_output/'+(image_name)+'.png')\n",
        "\n",
        "\n",
        "val_epoch_images = []\n",
        "val_epoch_segs = []\n",
        "val_epoch_preds = []\n"
      ],
      "id": "Yc39s5w-1IBX"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0C_1ReJ4voGs"
      },
      "source": [
        "#### Plot train and val losses"
      ],
      "id": "0C_1ReJ4voGs"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iu94oGakvoGs"
      },
      "outputs": [],
      "source": [
        "train_losses = [i['loss'] for i in metrics_train]\n",
        "val_losses = [i['loss'] for i in metrics_val]\n",
        "best_val_loss =min(val_losses)\n",
        "x = range(0, len(train_losses))\n",
        "\n",
        "fig, ax = plt.subplots(1, 1, figsize=(15, 15))\n",
        "ax.plot(x, train_losses, c='r',label='train')\n",
        "ax.plot(x, val_losses, c='b',label='val')\n",
        "ax.set_xlabel('Epoch', fontsize='xx-large')\n",
        "ax.set_ylabel('Dice loss', fontsize='xx-large')\n",
        "ax.legend(fontsize='xx-large')\n",
        "ax.text(110,0.9, f'Best Val Loss = {best_val_loss:.4}', fontsize='xx-large')\n",
        "\n",
        "#image_name = \"train_val_loss_graph_20220417-091638_epoch_157\"\n",
        "#plt.savefig('/content/drive/My Drive/cmr-haste-main/results/'+(image_name)+'.png')\n"
      ],
      "id": "iu94oGakvoGs"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwWLEHUHvoGs"
      },
      "source": [
        "#### Load the best model and evaluate on the test set"
      ],
      "id": "xwWLEHUHvoGs"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8r9_CdzPvoGs"
      },
      "outputs": [],
      "source": [
        "ckpt = ('/content/drive/My Drive/cmr-haste-main/output/20220417-091638-seg-haste/checkpoints/model_best_epoch_157.pth')\n",
        "\n",
        "model_best = utils.load_checkpoint(model, ckpt)\n",
        "model = model.cuda()"
      ],
      "id": "8r9_CdzPvoGs"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "vBHwwpJ1voGt"
      },
      "outputs": [],
      "source": [
        "loss_fn_test = DiceLoss(mode='binary', reduce=False)"
      ],
      "id": "vBHwwpJ1voGt"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "RgOTuyxRvoGt"
      },
      "outputs": [],
      "source": [
        "predict_seg = M.transforms.Compose([\n",
        "    M.transforms.Activations(sigmoid=True),\n",
        "    M.transforms.AsDiscrete(logit_thresh=0.5),\n",
        "    M.transforms.KeepLargestConnectedComponent(applied_labels=[1], connectivity=1),\n",
        "    M.transforms.FillHoles(applied_labels=[1], connectivity=1),\n",
        "])\n",
        "\n",
        "post_transforms = M.transforms.Compose([\n",
        "    M.transforms.ToDeviceD(keys=['image', 'seg', 'output', 'loss', 'pred_seg' ,'dice'], device='cpu'),\n",
        "    M.transforms.AsChannelLastD(keys=['image', 'seg', 'output', 'pred_seg']),\n",
        "    M.transforms.ToNumpyD(keys=['image', 'seg', 'output', 'loss', 'pred_seg' ,'dice']),\n",
        "])\n",
        "\n",
        "decollate = M.transforms.Decollated()\n",
        "to_cpu = M.transforms.ToNumpyD(keys=['image', 'seg', 'output', 'loss', 'pred_seg' ,'dice'])"
      ],
      "id": "RgOTuyxRvoGt"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cSP2kOqOvoGt"
      },
      "outputs": [],
      "source": [
        "data_dicts = []\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for batch_idx, batch in enumerate(tqdm(dataloader_test)):\n",
        "        \n",
        "        image, seg = batch.pop('image').cuda(), batch.pop('seg').cuda()\n",
        "        output = model(image)\n",
        "        loss = loss_fn_test(output, seg)\n",
        "    \n",
        "        batch_data_dicts = decollate({\n",
        "            'image': image,\n",
        "            'seg': seg,\n",
        "            'output': output,\n",
        "            'loss': loss,\n",
        "            **batch,\n",
        "        })\n",
        "        \n",
        "        for data_dict in batch_data_dicts:\n",
        "            data_dict['pred_seg'] = predict_seg(data_dict['output'])\n",
        "            data_dict['dice'] = dice_score(data_dict['pred_seg'], data_dict['seg'])\n",
        "        batch_data_dicts = post_transforms(batch_data_dicts)\n",
        "        data_dicts.extend(batch_data_dicts)"
      ],
      "id": "cSP2kOqOvoGt"
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "6IEcjS9tvoGu"
      },
      "outputs": [],
      "source": [
        "for dd in data_dicts:\n",
        "  \n",
        "    img = np.moveaxis(dd.pop('image'), 0, 1)\n",
        "    seg = np.moveaxis(dd.pop('seg'), 0, 1).astype(np.uint8)\n",
        "    area_ventricles = np.sum(seg) * 1 / 100\n",
        "    \n",
        "    try:\n",
        "        contour_ventricles = findContours(seg, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)[0][0][:, 0]\n",
        "    except:\n",
        "        contour_ventricles = []\n",
        "    \n",
        "    pred_seg = np.moveaxis(dd.pop('pred_seg'), 0, 1).astype(np.uint8)\n",
        "    pred_area_ventricles = np.sum(pred_seg) * 1 / 100\n",
        "    \n",
        "    try:\n",
        "        pred_contour_ventricles = findContours(pred_seg, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)[0][0][:, 0]\n",
        "    except:\n",
        "        pred_contour_ventricles = []\n",
        "    \n",
        "    \n",
        "    dd['image'] = img\n",
        "    dd['seg'] = seg\n",
        "    dd['area_ventricles'] = area_ventricles\n",
        "    dd['contour_ventricles'] = contour_ventricles\n",
        "    \n",
        "    dd['pred_seg'] = pred_seg\n",
        "    dd['pred_area_ventricles'] = pred_area_ventricles\n",
        "    dd['pred_contour_ventricles'] = pred_contour_ventricles\n",
        "    "
      ],
      "id": "6IEcjS9tvoGu"
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "XpwE_FcavoGv"
      },
      "outputs": [],
      "source": [
        "study_dicts = utils.groupby(data_dicts, key=lambda x: x['meta']['study_name'])\n",
        "study_dicts = {k: sorted(v, key=lambda x: x['meta']['image_number']) for k, v in study_dicts.items()}"
      ],
      "id": "XpwE_FcavoGv"
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "knXBXPjPvoGv"
      },
      "outputs": [],
      "source": [
        "for study_name, dds in study_dicts.items():\n",
        "    slice_max = max([i['meta']['slice_coord'] for i in dds])\n",
        "    slice_min = min([i['meta']['slice_coord'] for i in dds])\n",
        "    for dd in dds:\n",
        "        dd['meta']['slice_coord_norm'] = (dd['meta']['slice_coord'] - slice_min) / (slice_max - slice_min)"
      ],
      "id": "knXBXPjPvoGv"
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "s4GLG3ypvoGv"
      },
      "outputs": [],
      "source": [
        "results = []\n",
        "for studyname, dds in study_dicts.items():\n",
        "    for dd in dds:\n",
        "        results.append({\n",
        "            'image_uid': dd['meta']['image_uid'],\n",
        "            'area_ventricles': dd['area_ventricles'],\n",
        "            'pred_area_ventricles': dd['pred_area_ventricles'],\n",
        "            'dice': dd['dice'][0],\n",
        "            'image_number': dd['meta']['image_number'],\n",
        "            'slice_coord_norm': dd['meta']['slice_coord_norm'],\n",
        "            'series_number': dd['meta']['series_number'],\n",
        "            'sequence': dd['meta']['sequence'],\n",
        "            'study_uid': dd['meta']['study_uid'],\n",
        "            'study_name': dd['meta']['study_name'],\n",
        "            'cohort': dd['meta']['cohort'],\n",
        "        })\n",
        "results = pd.DataFrame(results)"
      ],
      "id": "s4GLG3ypvoGv"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hgfQC11voGw"
      },
      "source": [
        "##### Plot figures"
      ],
      "id": "9hgfQC11voGw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIpymqxjvoGw"
      },
      "outputs": [],
      "source": [
        "mean_dice = np.mean(results.dice)\n",
        "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
        "ax.hist(results.dice, bins=30, color='grey')\n",
        "ax.axvline(mean_dice, c='blue', linestyle='--')\n",
        "ax.text(0.63, 9, f'Avg = {mean_dice:.2}', fontsize='x-large')\n",
        "ax.set_xlabel('Dice', fontsize='x-large')\n",
        "#plt.show()\n",
        "\n",
        "#image_name = \"dice_graph_20220417-091638_epoch_157\"\n",
        "#plt.savefig('/content/drive/My Drive/cmr-haste-main/results/'+(image_name)+'.png')"
      ],
      "id": "OIpymqxjvoGw"
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "S10r7-jDvoGx"
      },
      "outputs": [],
      "source": [
        "results['area_ventricles_diff'] = results['area_ventricles'] - results['pred_area_ventricles']\n",
        "results['area_ventricles_avg'] = (results['area_ventricles'] + results['pred_area_ventricles']) / 2.0\n",
        "avg = np.mean(results['area_ventricles_diff'])\n",
        "std = np.std(results['area_ventricles_diff'])\n",
        "corr = np.corrcoef(results['area_ventricles'], results['pred_area_ventricles'])[0][1]"
      ],
      "id": "S10r7-jDvoGx"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4oYUXwo0voGx"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
        "\n",
        "# Scatter plot of area vs predicted area\n",
        "axes[0].plot(results['area_ventricles'], results['area_ventricles'], c='grey', linestyle='-')\n",
        "im0=axes[0].scatter(results['area_ventricles'], results['pred_area_ventricles'], c=results.slice_coord_norm)\n",
        "axes[0].set_xticks(range(0, 100, 10))\n",
        "axes[0].set_yticks(range(0, 100, 10))\n",
        "axes[0].text(2, 85, f'Corr. Coef = {corr:.2}', fontsize='x-large')\n",
        "axes[0].set_xlabel('Area', fontsize='x-large')\n",
        "axes[0].set_ylabel('Predicted Area', fontsize='x-large')\n",
        "\n",
        "# Bland-Altman plot\n",
        "im1=axes[1].scatter(results['area_ventricles_avg'], results['area_ventricles_diff'], c=results.slice_coord_norm)\n",
        "axes[1].axhline(avg, c='grey')\n",
        "axes[1].axhline(avg-1.96*std, c='grey', linestyle='--')\n",
        "axes[1].axhline(avg+1.96*std, c='grey', linestyle='--')\n",
        "axes[1].set_yticks(range(0, 40, 5))\n",
        "axes[1].set_xlabel('Average', fontsize='x-large')\n",
        "axes[1].set_ylabel('Area - Predicted Area', fontsize='x-large')\n",
        "\n",
        "fig.colorbar(im0, ax=axes[0])\n",
        "fig.colorbar(im1, ax=axes[1])\n",
        "\n",
        "#image_name = \"scatter_graph_20220417-091638_epoch_157\"\n",
        "#plt.savefig('/content/drive/My Drive/cmr-haste-main/results/'+(image_name)+'.png')"
      ],
      "id": "4oYUXwo0voGx"
    },
    {
      "cell_type": "code",
      "source": [
        "studies = list(study_dicts.keys())"
      ],
      "metadata": {
        "id": "NBk2zlRZFBvW"
      },
      "id": "NBk2zlRZFBvW",
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "median_dice = np.median(results.dice)\n",
        "q1,q3 = np.percentile(results.dice,[25,75])\n",
        "\n",
        "study_0 = studies[0]\n",
        "study_1 = studies[1]\n",
        "study_2 = studies[2]\n",
        "\n",
        "dice_vals_0 = results.loc[results['study_name']==study_0,['dice']]\n",
        "slice_vals_0 = results.loc[results['study_name']==study_0,['slice_coord_norm']]\n",
        "\n",
        "dice_vals_1 = results.loc[results['study_name']==study_1,['dice']]\n",
        "slice_vals_1 = results.loc[results['study_name']==study_1,['slice_coord_norm']]\n",
        "\n",
        "dice_vals_2 = results.loc[results['study_name']==study_2,['dice']]\n",
        "slice_vals_2 = results.loc[results['study_name']==study_2,['slice_coord_norm']]\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
        "#im = ax.scatter(results['slice_coord_norm'],results['dice'], c=results.slice_coord_norm)\n",
        "im = ax.scatter(slice_vals_0,dice_vals_0, c='#eb347d', label = \"Study 0\")\n",
        "im = ax.scatter(slice_vals_1,dice_vals_1, c='#34ebd9', label = \"Study 1\")\n",
        "im = ax.scatter(slice_vals_2,dice_vals_2, c='#abeb34', label = \"Study 2\")\n",
        "\n",
        "ax.axhline(y=median_dice, c='gray', linestyle='-')\n",
        "ax.axhline(y=q1, c='gray', linestyle='--')\n",
        "ax.axhline(y=q3, c='gray', linestyle='--')\n",
        "ax.set_xlabel('Normalised Slice Co-ordinate', fontsize='x-large')\n",
        "ax.set_ylabel('Dice Score', fontsize='x-large')\n",
        "\n",
        "ax.legend(loc='lower right')\n",
        "\n",
        "#image_name = \"dice analysis\"\n",
        "#plt.savefig('/content/drive/My Drive/cmr-haste-main/results/'+(image_name)+'.png')"
      ],
      "metadata": {
        "id": "_iQzPo_6RzRx"
      },
      "id": "_iQzPo_6RzRx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To plot all predictions for single study\n",
        "study = studies[0]\n",
        "#num_slices = print(len(study_dicts[study]))\n",
        "\n",
        "#for slice_idx in range(0,num_slices):\n",
        "fig, axes = plt.subplots(5,5, figsize=(25,25))\n",
        "try:\n",
        "  for idx, ax in enumerate(axes.flatten()):\n",
        "      ax.imshow(study_dicts[study][idx]['image'].squeeze(), cmap='gray')\n",
        "      ax.imshow(study_dicts[study][idx]['seg'].squeeze(), alpha =0.3)\n",
        "      #ax.plot(study_dicts[study][idx]['contour_ventricles'][:,1], study_dicts[study][idx]['contour_ventricles'][:,0], c='r')\n",
        "      ax.plot(study_dicts[study][idx]['pred_contour_ventricles'][:, 0], study_dicts[study][idx]['pred_contour_ventricles'][:, 1], c='r')\n",
        "      \n",
        "      dice = (study_dicts[study][idx]['dice'][0])\n",
        "      \n",
        "      ax.set_title((str(2))+\", img \"+str(idx)+\", \"+ f'Dice: {dice:.3}' ,fontsize='xx-large')\n",
        "      ax.set_axis_off()\n",
        "except IndexError:\n",
        "  pass\n",
        "\n",
        "#image_name = (\"test_preds_2_\"+(str(study)))\n",
        "#plt.savefig('/content/drive/My Drive/cmr-haste-main/results/'+(image_name)+'.png')"
      ],
      "metadata": {
        "id": "SRBnFdQNFyjj"
      },
      "id": "SRBnFdQNFyjj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oj9g5LrJvoGy"
      },
      "outputs": [],
      "source": [
        "study = studies[0]\n",
        "slice_idx = 15\n",
        "fig, axes = plt.subplots(1, 2, figsize=(20,20))\n",
        "axes[0].imshow(study_dicts[study][slice_idx]['image'].squeeze(), cmap='gray')\n",
        "axes[0].plot(study_dicts[study][slice_idx]['contour_ventricles'][:, 0], study_dicts[study][slice_idx]['contour_ventricles'][:, 1], c='r')\n",
        "axes[1].imshow(study_dicts[study][slice_idx]['image'].squeeze(), cmap='gray')\n",
        "axes[1].plot(study_dicts[study][slice_idx]['pred_contour_ventricles'][:, 0], study_dicts[study][slice_idx]['pred_contour_ventricles'][:, 1], c='r')\n",
        "axes[0].set_axis_off()\n",
        "axes[1].set_axis_off()\n",
        "\n",
        "#image_name = (\"test_2_worst\"+(str(study)))\n",
        "#plt.savefig('/content/drive/My Drive/cmr-haste-main/results/'+(image_name)+'.png')"
      ],
      "id": "Oj9g5LrJvoGy"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "AksFutUJvoGn",
        "6N_ExoJevoGo",
        "VulpV6xVvoGp"
      ],
      "name": "ECGI-CMR Segmentation Model .ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}